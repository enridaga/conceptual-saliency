{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Userv Survey answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import krippendorff\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fleiss' kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data_filtered into two DataFrames based on group type\n",
    "group_a_data = data[data['Group'] == 'a']\n",
    "group_b_data = data[data['Group'] == 'b']\n",
    "\n",
    "# Function to convert data into a format suitable for Fleiss' kappa\n",
    "def prepare_fleiss_data(df, column_name, n_raters):\n",
    "    # Create a contingency table\n",
    "    contingency_table = df.groupby(['Question Number', column_name]).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Ensure all categories are present\n",
    "    for category in ['strong top', 'weak top', 'equal', 'weak bottom', 'strong bottom']:\n",
    "        if category not in contingency_table.columns:\n",
    "            contingency_table[category] = 0\n",
    "    \n",
    "    # Sort columns to ensure consistent order\n",
    "    contingency_table = contingency_table[['strong top', 'weak top', 'equal', 'weak bottom', 'strong bottom']]\n",
    "    # print(contingency_table)\n",
    "    \n",
    "    # Ensure each question has n_raters ratings\n",
    "    for idx in contingency_table.index:\n",
    "        total_ratings = contingency_table.loc[idx].sum()\n",
    "        # print('total ratings', idx, total_ratings)\n",
    "        if total_ratings < n_raters:\n",
    "            contingency_table.loc[idx, 'fill'] = n_raters - total_ratings\n",
    "        elif total_ratings > n_raters:\n",
    "            contingency_table = contingency_table.drop(idx)\n",
    "    \n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' kappa results for Group A:\n",
      "   Rep Saliency Kappa  Pro Saliency Kappa\n",
      "0            0.160812            0.014687\n",
      "Fleiss' kappa results for Group B:\n",
      "   Rep Saliency Kappa  Pro Saliency Kappa\n",
      "0            0.073491           -0.061552\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate Fleiss' kappa for a group\n",
    "def calculate_fleiss_kappa_for_group(group_data, n_raters):\n",
    "    fleiss_kappa_results = []\n",
    "\n",
    "    # Prepare data for Rep Saliency and Pro Saliency\n",
    "    rep_contingency_table = prepare_fleiss_data(group_data, 'Rep Saliency', n_raters)\n",
    "    pro_contingency_table = prepare_fleiss_data(group_data, 'Pro Saliency', n_raters)\n",
    "    \n",
    "    # Calculate Fleiss' kappa for Rep Saliency\n",
    "    rep_fleiss_kappa = fleiss_kappa(rep_contingency_table.to_numpy())\n",
    "    \n",
    "    # Calculate Fleiss' kappa for Pro Saliency\n",
    "    pro_fleiss_kappa = fleiss_kappa(pro_contingency_table.to_numpy())\n",
    "    \n",
    "    fleiss_kappa_results.append({\n",
    "        'Rep Saliency Kappa': rep_fleiss_kappa,\n",
    "        'Pro Saliency Kappa': pro_fleiss_kappa\n",
    "    })\n",
    "    \n",
    "    return fleiss_kappa_results\n",
    "\n",
    "# Calculate Fleiss' kappa for group A (5 raters)\n",
    "group_a_kappa_results = calculate_fleiss_kappa_for_group(group_a_data, 5)\n",
    "\n",
    "# Calculate Fleiss' kappa for group B (3 raters)\n",
    "group_b_kappa_results = calculate_fleiss_kappa_for_group(group_b_data, 3)\n",
    "\n",
    "# Combine the results\n",
    "all_kappa_results = {\n",
    "    'Group A': group_a_kappa_results,\n",
    "    'Group B': group_b_kappa_results\n",
    "}\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "group_a_kappa_df = pd.DataFrame(group_a_kappa_results)\n",
    "group_b_kappa_df = pd.DataFrame(group_b_kappa_results)\n",
    "\n",
    "print(\"Fleiss' kappa results for Group A:\")\n",
    "print(group_a_kappa_df)\n",
    "\n",
    "print(\"Fleiss' kappa results for Group B:\")\n",
    "print(group_b_kappa_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_folder = '../evaluation'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "group_a_kappa_df.to_csv(os.path.join(output_folder, 'general_fleiss_kappa_results_group_a.csv'), index=False)\n",
    "group_b_kappa_df.to_csv(os.path.join(output_folder, 'general_fleiss_kappa_results_group_b.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Krippendorff's alpha: 0.10844421118543002\n"
     ]
    }
   ],
   "source": [
    "# Combine group and question number to ensure unique question identifiers\n",
    "data['Question ID'] = data['Group'] + '-' + data['Question Number'].astype(str)\n",
    "\n",
    "# Create a matrix of item-category counts\n",
    "categories = ['strong bottom', 'weak bottom', 'equal', 'weak top', 'strong top']\n",
    "category_mapping = {category: i for i, category in enumerate(categories)}\n",
    "\n",
    "# Gather all ratings into a list of lists\n",
    "ratings = []\n",
    "\n",
    "for question_id in data['Question ID'].unique():\n",
    "    question_ratings = data[data['Question ID'] == question_id][['Rep Saliency', 'Pro Saliency']].values.flatten()\n",
    "    ratings.append([category_mapping.get(rating, -1) for rating in question_ratings])\n",
    "    \n",
    "\n",
    "# Determine the maximum length of ratings\n",
    "max_len = max(len(row) for row in ratings)\n",
    "\n",
    "# Pad each row to the maximum length with -1\n",
    "padded_ratings = [row + [-1] * (max_len - len(row)) for row in ratings]\n",
    "\n",
    "# Convert to numpy array\n",
    "ratings_array = np.array(padded_ratings)\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "alpha = krippendorff.alpha(reliability_data=ratings_array, level_of_measurement='nominal')\n",
    "print(f'General Krippendorff\\'s alpha: {alpha}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha for Rep Saliency: 0.1066581472583652\n",
      "Krippendorff's alpha for Pro Saliency: 0.10878203315274781\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping for categories\n",
    "categories = ['strong bottom', 'weak bottom', 'equal', 'weak top', 'strong top']\n",
    "category_mapping = {category: i for i, category in enumerate(categories)}\n",
    "\n",
    "def prepare_ratings(saliency_column):\n",
    "    # Gather all ratings into a list of lists\n",
    "    ratings = []\n",
    "    for question_id in data['Question ID'].unique():\n",
    "        question_ratings = data[data['Question ID'] == question_id][saliency_column].values\n",
    "        ratings.append([category_mapping.get(rating, -1) for rating in question_ratings])\n",
    "    \n",
    "    # Determine the maximum length of ratings\n",
    "    max_len = max(len(row) for row in ratings)\n",
    "\n",
    "    # Pad each row to the maximum length with -1\n",
    "    padded_ratings = [row + [-1] * (max_len - len(row)) for row in ratings]\n",
    "\n",
    "    # Convert to numpy array\n",
    "    ratings_array = np.array(padded_ratings)\n",
    "    \n",
    "    return ratings_array\n",
    "\n",
    "# Prepare ratings for Rep Saliency\n",
    "rep_ratings_array = prepare_ratings('Rep Saliency')\n",
    "\n",
    "# Calculate Krippendorff's alpha for Rep Saliency\n",
    "alpha_rep = krippendorff.alpha(reliability_data=rep_ratings_array, level_of_measurement='nominal')\n",
    "print(f'Krippendorff\\'s alpha for Rep Saliency: {alpha_rep}')\n",
    "\n",
    "# Prepare ratings for Pro Saliency\n",
    "pro_ratings_array = prepare_ratings('Pro Saliency')\n",
    "\n",
    "# Calculate Krippendorff's alpha for Pro Saliency\n",
    "alpha_pro = krippendorff.alpha(reliability_data=pro_ratings_array, level_of_measurement='nominal')\n",
    "print(f'Krippendorff\\'s alpha for Pro Saliency: {alpha_pro}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote\n",
    "We want to see the **majority vote**. Divided by group (A and B), for each question we check the answers of users, and get the agreement of the majority. In case of a tie, we take the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_24092\\715546659.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  majority_votes = grouped.apply(lambda x: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('../evaluation/filtered_saliency_survey.csv')\n",
    "\n",
    "# Map saliency levels to broader categories\n",
    "saliency_mapping = {\n",
    "    'strong top': 'top',\n",
    "    'weak top': 'top',\n",
    "    'equal': 'equal',\n",
    "    'weak bottom': 'bottom',\n",
    "    'strong bottom': 'bottom'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the saliency columns\n",
    "data['Rep Saliency Merged'] = data['Rep Saliency'].map(saliency_mapping)\n",
    "data['Pro Saliency Merged'] = data['Pro Saliency'].map(saliency_mapping)\n",
    "\n",
    "# Save the majority votes to a CSV file\n",
    "# output_folder = '../evaluation/majority/by_group'\n",
    "output_folder = '../evaluation'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to determine the majority vote\n",
    "def majority_vote(saliency_list):\n",
    "    count = Counter(saliency_list)\n",
    "    most_common = count.most_common()\n",
    "    if len(most_common) == 1:\n",
    "        return most_common[0][0]\n",
    "    if most_common[0][1] > most_common[1][1]:\n",
    "        return most_common[0][0]\n",
    "    # Handle tie by choosing the worst\n",
    "    tie_candidates = [item for item in most_common if item[1] == most_common[0][1]]\n",
    "    priority = {'top': 1, 'equal': 0, 'bottom': -1}\n",
    "    return min(tie_candidates, key=lambda x: priority[x[0]])[0]\n",
    "\n",
    "# Group by Method, Chart, and Question Number\n",
    "grouped = data.groupby(['Method', 'Chart', 'Question Number', 'Group'])\n",
    "\n",
    "# Extract the majority vote for each question\n",
    "majority_votes = grouped.apply(lambda x: pd.Series({\n",
    "    'Rep Majority': majority_vote(x['Rep Saliency Merged']),\n",
    "    'Pro Majority': majority_vote(x['Pro Saliency Merged'])\n",
    "})).reset_index()\n",
    "\n",
    "majority_votes.to_csv(os.path.join(output_folder, 'saliency_majority_votes.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden standard\n",
    "For each group, given all the answers from users in the majority data frame, create two lists, one per \"representative\" and one for \"prominent\" for a golden standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group A Question Dictionary:\n",
      "{1: {'top': 1, 'bottom': 53}, 2: {'top': 35, 'bottom': 44}, 3: {'top': 32, 'bottom': 13}, 4: {'top': 56, 'bottom': 77}, 5: {'top': 1, 'bottom': 65}, 6: {'top': 79, 'bottom': 71}, 7: {'top': 40, 'bottom': 5}, 8: {'top': 4, 'bottom': 62}, 9: {'top': 7, 'bottom': 8}, 10: {'top': 26, 'bottom': 47}, 11: {'top': 40, 'bottom': 39}, 12: {'top': 35, 'bottom': 10}, 13: {'top': 7, 'bottom': 65}, 14: {'top': 35, 'bottom': 53}, 15: {'top': 35, 'bottom': 47}, 16: {'top': 32, 'bottom': 39}, 17: {'top': 1, 'bottom': 77}, 18: {'top': 79, 'bottom': 5}, 19: {'top': 4, 'bottom': 10}, 20: {'top': 79, 'bottom': 77}, 21: {'top': 79, 'bottom': 13}, 22: {'top': 56, 'bottom': 39}, 23: {'top': 56, 'bottom': 71}, 24: {'top': 56, 'bottom': 65}, 25: {'top': 79, 'bottom': 8}, 26: {'top': 36, 'bottom': 16}, 27: {'top': 82, 'bottom': 6}, 28: {'top': 29, 'bottom': 16}, 29: {'top': 42, 'bottom': 72}, 30: {'top': 21, 'bottom': 60}, 31: {'top': 26, 'bottom': 2}, 32: {'top': 67, 'bottom': 68}, 33: {'top': 38, 'bottom': 76}, 34: {'top': 74, 'bottom': 14}, 35: {'top': 84, 'bottom': 54}, 36: {'top': 3, 'bottom': 54}, 37: {'top': 41, 'bottom': 68}, 38: {'top': 32, 'bottom': 61}, 39: {'top': 85, 'bottom': 61}, 40: {'top': 39, 'bottom': 76}, 41: {'top': 24, 'bottom': 2}, 42: {'top': 80, 'bottom': 14}, 43: {'top': 81, 'bottom': 6}, 44: {'top': 49, 'bottom': 72}, 45: {'top': 78, 'bottom': 16}, 46: {'top': 43, 'bottom': 2}, 47: {'top': 44, 'bottom': 60}, 48: {'top': 50, 'bottom': 61}, 49: {'top': 34, 'bottom': 54}, 50: {'top': 37, 'bottom': 76}, 51: {'top': 53, 'bottom': 82}, 52: {'top': 5, 'bottom': 27}, 53: {'top': 5, 'bottom': 35}, 54: {'top': 44, 'bottom': 62}, 55: {'top': 9, 'bottom': 40}, 56: {'top': 8, 'bottom': 57}, 57: {'top': 64, 'bottom': 37}, 58: {'top': 10, 'bottom': 43}, 59: {'top': 11, 'bottom': 81}, 60: {'top': 44, 'bottom': 14}, 61: {'top': 8, 'bottom': 66}, 62: {'top': 4, 'bottom': 67}, 63: {'top': 15, 'bottom': 60}, 64: {'top': 64, 'bottom': 78}, 65: {'top': 4, 'bottom': 50}, 66: {'top': 20, 'bottom': 16}, 67: {'top': 20, 'bottom': 55}, 68: {'top': 53, 'bottom': 31}, 69: {'top': 15, 'bottom': 80}, 70: {'top': 9, 'bottom': 69}, 71: {'top': 10, 'bottom': 49}, 72: {'top': 11, 'bottom': 7}, 73: {'top': 64, 'bottom': 56}, 74: {'top': 20, 'bottom': 25}, 75: {'top': 20, 'bottom': 37}, 76: {'top': 75, 'bottom': 6}, 77: {'top': 66, 'bottom': 3}, 78: {'top': 22, 'bottom': 4}, 79: {'top': 57, 'bottom': 2}, 80: {'top': 50, 'bottom': 3}, 81: {'top': 37, 'bottom': 5}, 82: {'top': 43, 'bottom': 2}, 83: {'top': 49, 'bottom': 5}, 84: {'top': 78, 'bottom': 4}, 85: {'top': 65, 'bottom': 6}, 86: {'top': 55, 'bottom': 4}, 87: {'top': 77, 'bottom': 5}, 88: {'top': 81, 'bottom': 6}, 89: {'top': 69, 'bottom': 3}, 90: {'top': 62, 'bottom': 2}, 91: {'top': 75, 'bottom': 5}, 92: {'top': 77, 'bottom': 3}, 93: {'top': 69, 'bottom': 4}, 94: {'top': 43, 'bottom': 4}, 95: {'top': 55, 'bottom': 6}, 96: {'top': 57, 'bottom': 6}, 97: {'top': 65, 'bottom': 3}, 98: {'top': 78, 'bottom': 2}, 99: {'top': 37, 'bottom': 4}, 100: {'top': 50, 'bottom': 5}, 101: {'top': 27, 'bottom': 77}, 102: {'top': 14, 'bottom': 55}, 103: {'top': 4, 'bottom': 74}, 104: {'top': 7, 'bottom': 81}, 105: {'top': 11, 'bottom': 74}, 106: {'top': 8, 'bottom': 75}, 107: {'top': 16, 'bottom': 82}, 108: {'top': 15, 'bottom': 82}, 109: {'top': 10, 'bottom': 80}, 110: {'top': 31, 'bottom': 2}, 111: {'top': 17, 'bottom': 49}, 112: {'top': 24, 'bottom': 1}, 113: {'top': 18, 'bottom': 78}, 114: {'top': 9, 'bottom': 77}, 115: {'top': 5, 'bottom': 64}, 116: {'top': 28, 'bottom': 80}, 117: {'top': 21, 'bottom': 64}, 118: {'top': 13, 'bottom': 49}, 119: {'top': 12, 'bottom': 2}, 120: {'top': 26, 'bottom': 55}, 121: {'top': 33, 'bottom': 75}, 122: {'top': 6, 'bottom': 1}, 123: {'top': 33, 'bottom': 1}, 124: {'top': 28, 'bottom': 74}, 125: {'top': 4, 'bottom': 77}, 126: {'top': 34, 'bottom': 56}, 127: {'top': 29, 'bottom': 14}, 128: {'top': 64, 'bottom': 14}, 129: {'top': 5, 'bottom': 1}, 130: {'top': 8, 'bottom': 56}, 131: {'top': 63, 'bottom': 6}, 132: {'top': 83, 'bottom': 14}, 133: {'top': 26, 'bottom': 1}, 134: {'top': 84, 'bottom': 1}, 135: {'top': 10, 'bottom': 2}, 136: {'top': 9, 'bottom': 14}, 137: {'top': 46, 'bottom': 3}, 138: {'top': 20, 'bottom': 3}, 139: {'top': 22, 'bottom': 3}, 140: {'top': 70, 'bottom': 6}, 141: {'top': 19, 'bottom': 14}, 142: {'top': 72, 'bottom': 2}, 143: {'top': 53, 'bottom': 6}, 144: {'top': 58, 'bottom': 3}, 145: {'top': 51, 'bottom': 56}, 146: {'top': 4, 'bottom': 6}, 147: {'top': 45, 'bottom': 3}, 148: {'top': 52, 'bottom': 1}, 149: {'top': 41, 'bottom': 2}, 150: {'top': 7, 'bottom': 56}, 151: {'top': 18, 'bottom': 57}, 152: {'top': 27, 'bottom': 77}, 153: {'top': 43, 'bottom': 80}, 154: {'top': 25, 'bottom': 16}, 155: {'top': 31, 'bottom': 55}, 156: {'top': 28, 'bottom': 49}, 157: {'top': 61, 'bottom': 7}, 158: {'top': 60, 'bottom': 62}, 159: {'top': 12, 'bottom': 56}, 160: {'top': 15, 'bottom': 66}, 161: {'top': 23, 'bottom': 24}, 162: {'top': 61, 'bottom': 14}, 163: {'top': 48, 'bottom': 75}, 164: {'top': 11, 'bottom': 42}, 165: {'top': 25, 'bottom': 1}, 166: {'top': 36, 'bottom': 69}, 167: {'top': 68, 'bottom': 78}, 168: {'top': 83, 'bottom': 9}, 169: {'top': 52, 'bottom': 81}, 170: {'top': 39, 'bottom': 30}, 171: {'top': 33, 'bottom': 67}, 172: {'top': 13, 'bottom': 6}, 173: {'top': 27, 'bottom': 4}, 174: {'top': 68, 'bottom': 2}, 175: {'top': 44, 'bottom': 74}, 176: {'top': 82, 'bottom': 59}, 177: {'top': 82, 'bottom': 5}, 178: {'top': 82, 'bottom': 58}, 179: {'top': 82, 'bottom': 4}, 180: {'top': 81, 'bottom': 59}, 181: {'top': 81, 'bottom': 5}, 182: {'top': 81, 'bottom': 58}, 183: {'top': 81, 'bottom': 4}, 184: {'top': 80, 'bottom': 59}, 185: {'top': 80, 'bottom': 5}, 186: {'top': 80, 'bottom': 58}, 187: {'top': 80, 'bottom': 4}}\n",
      "\n",
      "Group B Question Dictionary:\n",
      "{1: {'top': 7, 'bottom': 47}, 2: {'top': 32, 'bottom': 8}, 3: {'top': 7, 'bottom': 5}, 4: {'top': 35, 'bottom': 39}, 5: {'top': 7, 'bottom': 53}, 6: {'top': 56, 'bottom': 47}, 7: {'top': 26, 'bottom': 65}, 8: {'top': 79, 'bottom': 39}, 9: {'top': 40, 'bottom': 44}, 10: {'top': 79, 'bottom': 53}, 11: {'top': 40, 'bottom': 71}, 12: {'top': 40, 'bottom': 47}, 13: {'top': 1, 'bottom': 71}, 14: {'top': 35, 'bottom': 77}, 15: {'top': 32, 'bottom': 62}, 16: {'top': 40, 'bottom': 62}, 17: {'top': 1, 'bottom': 62}, 18: {'top': 1, 'bottom': 47}, 19: {'top': 26, 'bottom': 39}, 20: {'top': 32, 'bottom': 47}, 21: {'top': 40, 'bottom': 13}, 22: {'top': 4, 'bottom': 8}, 23: {'top': 4, 'bottom': 13}, 24: {'top': 56, 'bottom': 5}, 25: {'top': 7, 'bottom': 77}, 26: {'top': 30, 'bottom': 6}, 27: {'top': 38, 'bottom': 2}, 28: {'top': 29, 'bottom': 72}, 29: {'top': 24, 'bottom': 60}, 30: {'top': 44, 'bottom': 6}, 31: {'top': 80, 'bottom': 60}, 32: {'top': 38, 'bottom': 60}, 33: {'top': 39, 'bottom': 14}, 34: {'top': 44, 'bottom': 76}, 35: {'top': 78, 'bottom': 68}, 36: {'top': 37, 'bottom': 68}, 37: {'top': 26, 'bottom': 72}, 38: {'top': 21, 'bottom': 16}, 39: {'top': 49, 'bottom': 61}, 40: {'top': 30, 'bottom': 60}, 41: {'top': 36, 'bottom': 72}, 42: {'top': 29, 'bottom': 6}, 43: {'top': 43, 'bottom': 16}, 44: {'top': 43, 'bottom': 6}, 45: {'top': 3, 'bottom': 60}, 46: {'top': 21, 'bottom': 14}, 47: {'top': 67, 'bottom': 60}, 48: {'top': 78, 'bottom': 6}, 49: {'top': 85, 'bottom': 14}, 50: {'top': 81, 'bottom': 60}, 51: {'top': 15, 'bottom': 37}, 52: {'top': 11, 'bottom': 14}, 53: {'top': 10, 'bottom': 82}, 54: {'top': 53, 'bottom': 35}, 55: {'top': 20, 'bottom': 57}, 56: {'top': 4, 'bottom': 57}, 57: {'top': 20, 'bottom': 50}, 58: {'top': 53, 'bottom': 25}, 59: {'top': 15, 'bottom': 49}, 60: {'top': 44, 'bottom': 82}, 61: {'top': 15, 'bottom': 78}, 62: {'top': 53, 'bottom': 43}, 63: {'top': 9, 'bottom': 57}, 64: {'top': 44, 'bottom': 57}, 65: {'top': 10, 'bottom': 81}, 66: {'top': 10, 'bottom': 27}, 67: {'top': 64, 'bottom': 50}, 68: {'top': 20, 'bottom': 56}, 69: {'top': 10, 'bottom': 78}, 70: {'top': 53, 'bottom': 40}, 71: {'top': 8, 'bottom': 60}, 72: {'top': 20, 'bottom': 14}, 73: {'top': 11, 'bottom': 37}, 74: {'top': 64, 'bottom': 69}, 75: {'top': 4, 'bottom': 49}, 76: {'top': 69, 'bottom': 5}, 77: {'top': 57, 'bottom': 3}, 78: {'top': 77, 'bottom': 4}, 79: {'top': 62, 'bottom': 4}, 80: {'top': 57, 'bottom': 5}, 81: {'top': 62, 'bottom': 6}, 82: {'top': 66, 'bottom': 2}, 83: {'top': 50, 'bottom': 2}, 84: {'top': 77, 'bottom': 2}, 85: {'top': 75, 'bottom': 2}, 86: {'top': 55, 'bottom': 2}, 87: {'top': 78, 'bottom': 3}, 88: {'top': 49, 'bottom': 2}, 89: {'top': 66, 'bottom': 4}, 90: {'top': 65, 'bottom': 4}, 91: {'top': 22, 'bottom': 2}, 92: {'top': 37, 'bottom': 6}, 93: {'top': 49, 'bottom': 3}, 94: {'top': 75, 'bottom': 3}, 95: {'top': 69, 'bottom': 6}, 96: {'top': 69, 'bottom': 2}, 97: {'top': 49, 'bottom': 4}, 98: {'top': 57, 'bottom': 4}, 99: {'top': 22, 'bottom': 3}, 100: {'top': 22, 'bottom': 5}, 101: {'top': 26, 'bottom': 1}, 102: {'top': 13, 'bottom': 1}, 103: {'top': 24, 'bottom': 81}, 104: {'top': 18, 'bottom': 49}, 105: {'top': 11, 'bottom': 78}, 106: {'top': 26, 'bottom': 74}, 107: {'top': 24, 'bottom': 55}, 108: {'top': 26, 'bottom': 75}, 109: {'top': 16, 'bottom': 55}, 110: {'top': 33, 'bottom': 74}, 111: {'top': 6, 'bottom': 64}, 112: {'top': 28, 'bottom': 55}, 113: {'top': 13, 'bottom': 82}, 114: {'top': 4, 'bottom': 78}, 115: {'top': 12, 'bottom': 75}, 116: {'top': 4, 'bottom': 81}, 117: {'top': 28, 'bottom': 75}, 118: {'top': 9, 'bottom': 64}, 119: {'top': 27, 'bottom': 81}, 120: {'top': 7, 'bottom': 82}, 121: {'top': 9, 'bottom': 75}, 122: {'top': 18, 'bottom': 55}, 123: {'top': 4, 'bottom': 2}, 124: {'top': 28, 'bottom': 1}, 125: {'top': 24, 'bottom': 2}, 126: {'top': 17, 'bottom': 1}, 127: {'top': 59, 'bottom': 56}, 128: {'top': 32, 'bottom': 2}, 129: {'top': 85, 'bottom': 2}, 130: {'top': 26, 'bottom': 6}, 131: {'top': 45, 'bottom': 56}, 132: {'top': 10, 'bottom': 56}, 133: {'top': 51, 'bottom': 6}, 134: {'top': 10, 'bottom': 3}, 135: {'top': 45, 'bottom': 14}, 136: {'top': 26, 'bottom': 3}, 137: {'top': 29, 'bottom': 1}, 138: {'top': 84, 'bottom': 6}, 139: {'top': 53, 'bottom': 1}, 140: {'top': 5, 'bottom': 14}, 141: {'top': 17, 'bottom': 6}, 142: {'top': 52, 'bottom': 14}, 143: {'top': 64, 'bottom': 3}, 144: {'top': 34, 'bottom': 14}, 145: {'top': 4, 'bottom': 56}, 146: {'top': 70, 'bottom': 2}, 147: {'top': 8, 'bottom': 3}, 148: {'top': 84, 'bottom': 56}, 149: {'top': 70, 'bottom': 14}, 150: {'top': 29, 'bottom': 56}, 151: {'top': 39, 'bottom': 50}, 152: {'top': 47, 'bottom': 37}, 153: {'top': 11, 'bottom': 21}, 154: {'top': 54, 'bottom': 82}, 155: {'top': 38, 'bottom': 40}, 156: {'top': 12, 'bottom': 5}, 157: {'top': 41, 'bottom': 65}, 158: {'top': 76, 'bottom': 8}, 159: {'top': 60, 'bottom': 35}, 160: {'top': 72, 'bottom': 3}, 161: {'top': 41, 'bottom': 10}, 162: {'top': 44, 'bottom': 67}, 163: {'top': 76, 'bottom': 62}, 164: {'top': 12, 'bottom': 82}, 165: {'top': 33, 'bottom': 16}, 166: {'top': 38, 'bottom': 2}, 167: {'top': 68, 'bottom': 67}, 168: {'top': 12, 'bottom': 9}, 169: {'top': 36, 'bottom': 16}, 170: {'top': 41, 'bottom': 9}, 171: {'top': 83, 'bottom': 14}, 172: {'top': 31, 'bottom': 65}, 173: {'top': 47, 'bottom': 49}, 174: {'top': 76, 'bottom': 55}, 175: {'top': 28, 'bottom': 77}, 176: {'top': 82, 'bottom': 59}, 177: {'top': 82, 'bottom': 5}, 178: {'top': 82, 'bottom': 58}, 179: {'top': 82, 'bottom': 4}, 180: {'top': 84, 'bottom': 59}, 181: {'top': 84, 'bottom': 5}, 182: {'top': 84, 'bottom': 58}, 183: {'top': 84, 'bottom': 4}, 184: {'top': 85, 'bottom': 59}, 185: {'top': 85, 'bottom': 5}, 186: {'top': 85, 'bottom': 58}, 187: {'top': 85, 'bottom': 4}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the origin file (assuming it's a JSON file)\n",
    "with open('../a_group_data.json', 'r') as file:\n",
    "    group_a_data = json.load(file)\n",
    "\n",
    "with open('../b_group_data.json', 'r') as file:\n",
    "    group_b_data = json.load(file)\n",
    "\n",
    "# Function to create the dictionary for a group\n",
    "def create_question_dict(group_data):\n",
    "    question_dict = {}\n",
    "    question_number = 1\n",
    "\n",
    "    for method, charts in group_data.items():\n",
    "        for chart, pairs in charts.items():\n",
    "            for pair in pairs:\n",
    "                top_concept_id = None\n",
    "                bottom_concept_id = None\n",
    "                \n",
    "                for cluster in pair:\n",
    "                    if cluster['saliency'] == 'top':\n",
    "                        top_concept_id = cluster['id']\n",
    "                    elif cluster['saliency'] == 'bottom':\n",
    "                        bottom_concept_id = cluster['id']\n",
    "                \n",
    "                question_dict[question_number] = {\n",
    "                    \"top\": top_concept_id,\n",
    "                    \"bottom\": bottom_concept_id\n",
    "                }\n",
    "                question_number += 1\n",
    "\n",
    "    return question_dict\n",
    "\n",
    "# Create the dictionaries for each group\n",
    "group_a_question_dict = create_question_dict(group_a_data)\n",
    "group_b_question_dict = create_question_dict(group_b_data)\n",
    "\n",
    "# Display the results\n",
    "print(\"Group A Question Dictionary:\")\n",
    "print(group_a_question_dict)\n",
    "\n",
    "print(\"\\nGroup B Question Dictionary:\")\n",
    "print(group_b_question_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative Golden List: {65, 1, 6, 38, 70, 12, 45, 50, 83, 21}\n",
      "Prominent Golden List: {7, 8, 13, 15, 18, 19, 22, 25, 30, 32, 36, 40, 45, 46, 52, 53, 65, 66, 69, 70, 72}\n",
      "Removed Representative List: {2, 3, 4, 5, 7, 8, 10, 11, 14, 15, 16, 18, 22, 27, 28, 31, 33, 37, 39, 47, 49, 54, 55, 57, 77, 78, 82, 84}\n",
      "Removed Prominent List: {2, 4, 5, 10, 14, 16, 24, 27, 29, 38, 39, 44, 49, 50, 54, 55, 57, 60, 61, 62, 64, 68, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84}\n",
      "Not Added Representative List: {4, 5, 7, 9, 11, 13, 14, 15, 16, 19, 20, 23, 24, 25, 26, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 48, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85}\n",
      "Not Added Prominent List: {1, 3, 4, 6, 9, 10, 11, 12, 14, 20, 26, 28, 33, 34, 35, 37, 39, 42, 43, 44, 47, 48, 50, 54, 55, 56, 58, 59, 60, 64, 67, 68, 71, 75, 76, 81, 82, 85}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../evaluation/saliency_majority_votes.csv')\n",
    "\n",
    "# Combine question dictionaries for easier access\n",
    "question_dict = {\n",
    "    'a': group_a_question_dict,\n",
    "    'b': group_b_question_dict\n",
    "}\n",
    "\n",
    "# Initialize global lists\n",
    "rep_list = []\n",
    "pro_list = []\n",
    "removed_rep_list = []\n",
    "removed_pro_list = []\n",
    "not_added_rep_list = []\n",
    "not_added_pro_list = []\n",
    "\n",
    "# Function to update the global lists based on the majority votes\n",
    "def update_golden_lists(df, question_dict):\n",
    "    global rep_list, pro_list, removed_rep_list, removed_pro_list, not_added_rep_list, not_added_pro_list\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        question_number = row['Question Number']\n",
    "        group = row['Group']\n",
    "        rep_majority = row['Rep Majority']\n",
    "        pro_majority = row['Pro Majority']\n",
    "\n",
    "        if rep_majority != 'equal':\n",
    "            rep_concept_id = question_dict[group][question_number][rep_majority]\n",
    "            rep_other_concept_id = question_dict[group][question_number]['bottom'] if rep_majority == 'top' else question_dict[group][question_number]['top']\n",
    "            if rep_concept_id not in rep_list and rep_concept_id not in removed_rep_list and rep_concept_id not in not_added_rep_list:\n",
    "                if rep_other_concept_id in rep_list:\n",
    "                    rep_list.remove(rep_other_concept_id)\n",
    "                    removed_rep_list.append(rep_other_concept_id)\n",
    "                    not_added_rep_list.append(rep_concept_id)\n",
    "                else:\n",
    "                    rep_list.append(rep_concept_id)\n",
    "\n",
    "        if pro_majority != 'equal':\n",
    "            pro_concept_id = question_dict[group][question_number][pro_majority]\n",
    "            pro_other_concept_id = question_dict[group][question_number]['bottom'] if pro_majority == 'top' else question_dict[group][question_number]['top']\n",
    "            if pro_concept_id not in pro_list and pro_concept_id not in removed_pro_list and pro_concept_id not in not_added_pro_list:\n",
    "                if pro_other_concept_id in pro_list:\n",
    "                    pro_list.remove(pro_other_concept_id)\n",
    "                    removed_pro_list.append(pro_other_concept_id)\n",
    "                    not_added_pro_list.append(pro_concept_id)\n",
    "                else:\n",
    "                    pro_list.append(pro_concept_id)\n",
    "\n",
    "        if rep_majority == 'equal':\n",
    "            rep_concept_id = question_dict[group][question_number]['top']\n",
    "            rep_other_concept_id = question_dict[group][question_number]['bottom']\n",
    "            if rep_concept_id in rep_list:\n",
    "                rep_list.remove(rep_concept_id)\n",
    "                removed_rep_list.append(rep_concept_id)\n",
    "            else:\n",
    "                not_added_rep_list.append(rep_concept_id)\n",
    "            if rep_other_concept_id in rep_list:\n",
    "                rep_list.remove(rep_other_concept_id)\n",
    "                removed_rep_list.append(rep_other_concept_id)\n",
    "            else:\n",
    "                not_added_rep_list.append(rep_other_concept_id) \n",
    "        if pro_majority == 'equal':\n",
    "            pro_concept_id = question_dict[group][question_number]['top']\n",
    "            pro_other_concept_id = question_dict[group][question_number]['bottom']\n",
    "            if pro_concept_id in pro_list:\n",
    "                pro_list.remove(pro_concept_id)\n",
    "                removed_pro_list.append(pro_concept_id)\n",
    "            else:\n",
    "                not_added_pro_list.append(pro_concept_id)\n",
    "            if pro_other_concept_id in pro_list:\n",
    "                pro_list.remove(pro_other_concept_id)\n",
    "                removed_pro_list.append(pro_other_concept_id)\n",
    "            else:\n",
    "                not_added_pro_list.append(pro_other_concept_id) \n",
    "\n",
    "# Separate the data by group\n",
    "group_a_data = data[data['Group'] == 'a']\n",
    "group_b_data = data[data['Group'] == 'b']\n",
    "\n",
    "# Update the global lists using the majority votes data\n",
    "update_golden_lists(group_a_data, question_dict)\n",
    "update_golden_lists(group_b_data, question_dict)\n",
    "\n",
    "# Display the results\n",
    "print(\"Representative Golden List:\", set(rep_list))\n",
    "print(\"Prominent Golden List:\", set(pro_list))\n",
    "print(\"Removed Representative List:\", set(removed_rep_list))\n",
    "print(\"Removed Prominent List:\", set(removed_pro_list))\n",
    "print(\"Not Added Representative List:\", set(not_added_rep_list))\n",
    "print(\"Not Added Prominent List:\", set(not_added_pro_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare how many of the concepts in the golden standards are also present in our top concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: VIG, Chart: lattice\n",
      "  Representative Intersection: {1}\n",
      "  Representative Percentage: 11.111%\n",
      "  Prominent Intersection: {40, 32, 7}\n",
      "  Prominent Percentage: 33.333%\n",
      "Method: VIG, Chart: scatterplot\n",
      "  Representative Intersection: {50, 21, 38}\n",
      "  Representative Percentage: 11.538%\n",
      "  Prominent Intersection: {32, 36, 30}\n",
      "  Prominent Percentage: 11.538%\n",
      "Method: SIG, Chart: lattice\n",
      "  Representative Intersection: set()\n",
      "  Representative Percentage: 0.000%\n",
      "  Prominent Intersection: {8, 53, 15}\n",
      "  Prominent Percentage: 27.273%\n",
      "Method: SIG, Chart: scatterplot\n",
      "  Representative Intersection: {65, 50}\n",
      "  Representative Percentage: 13.333%\n",
      "  Prominent Intersection: {65, 66, 69, 22}\n",
      "  Prominent Percentage: 26.667%\n",
      "Method: SIG, Chart: barchart\n",
      "  Representative Intersection: {12, 21, 6}\n",
      "  Representative Percentage: 13.636%\n",
      "  Prominent Intersection: {7, 8, 13, 15, 18}\n",
      "  Prominent Percentage: 22.727%\n",
      "Method: XRAY, Chart: lattice\n",
      "  Representative Intersection: {83, 45, 70}\n",
      "  Representative Percentage: 10.345%\n",
      "  Prominent Intersection: {32, 70, 7, 72, 8, 45, 46, 19, 52, 53, 22}\n",
      "  Prominent Percentage: 37.931%\n",
      "Method: XRAY, Chart: scatterplot\n",
      "  Representative Intersection: {83, 12, 38}\n",
      "  Representative Percentage: 11.111%\n",
      "  Prominent Intersection: {36, 72, 13, 15, 18, 52, 25}\n",
      "  Prominent Percentage: 25.926%\n",
      "Method: XRAY, Chart: barchart\n",
      "  Representative Intersection: set()\n",
      "  Representative Percentage: 0.000%\n",
      "  Prominent Intersection: set()\n",
      "  Prominent Percentage: 0.000%\n"
     ]
    }
   ],
   "source": [
    "# find intersection\n",
    "def intersection(lst1, lst2):\n",
    "    return set(lst1).intersection(lst2)\n",
    "\n",
    "methods = ['VIG', 'SIG', 'XRAY']\n",
    "charts = ['lattice', 'scatterplot', 'barchart']\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "intersection_results = {}\n",
    "\n",
    "\n",
    "# Initialize a list to store the results for CSV\n",
    "csv_results = []\n",
    "\n",
    "for method in methods:\n",
    "    # Load the JSON file for the current method\n",
    "    with open(f'../results/{method}_results.json', 'r') as file:\n",
    "        method_data = json.load(file)\n",
    "\n",
    "    for chart in charts:\n",
    "        if chart in method_data and '01' in method_data[chart] and len(method_data[chart]['01']) > 0:\n",
    "            combination_top = method_data[chart]['01']\n",
    "            representative_intersection = intersection(combination_top, rep_list)\n",
    "            prominent_intersection = intersection(combination_top, pro_list)\n",
    "\n",
    "            # Calculate the percentage of intersection\n",
    "            rep_percentage = round((len(representative_intersection) / len(combination_top)) * 100, 3)\n",
    "            pro_percentage = round((len(prominent_intersection) / len(combination_top)) * 100, 3)\n",
    "\n",
    "            # Store the results in a dictionary\n",
    "            result = {\n",
    "                'Method': method,\n",
    "                'Chart': chart,\n",
    "                'Representative Intersection': list(representative_intersection),\n",
    "                'Prominent Intersection': list(prominent_intersection),\n",
    "                'Representative Percentage': rep_percentage,\n",
    "                'Prominent Percentage': pro_percentage\n",
    "            }\n",
    "\n",
    "            # Add to the list for CSV\n",
    "            csv_results.append(result)\n",
    "\n",
    "            # Print the results\n",
    "            print(f\"Method: {method}, Chart: {chart}\")\n",
    "            print(f\"  Representative Intersection: {representative_intersection}\")\n",
    "            print(f\"  Representative Percentage: {rep_percentage:.3f}%\")\n",
    "            print(f\"  Prominent Intersection: {prominent_intersection}\")\n",
    "            print(f\"  Prominent Percentage: {pro_percentage:.3f}%\")\n",
    "\n",
    "# Save the results to a new JSON file\n",
    "output_json_path = '../evaluation/golden_intersection/intersection_results.json'\n",
    "with open(output_json_path, 'w') as outfile:\n",
    "    json.dump(csv_results, outfile, indent=4)\n",
    "\n",
    "# Convert the results to a DataFrame and save as CSV\n",
    "df = pd.DataFrame(csv_results)\n",
    "output_csv_path = '../evaluation/golden_intersection/intersection_results.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
